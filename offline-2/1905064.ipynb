{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***PRE PROCESSING***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_1_preprocessing():\n",
    "    df = pd.read_csv('datasets/telco_churn.csv')\n",
    "    target_column_name='Churn'\n",
    "    df = df.drop(columns=['customerID'])\n",
    "    # change data type of TotalCharges to float\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "\n",
    "    df = df.dropna(subset=[target_column_name])\n",
    "    df = df.drop_duplicates()\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    categorical_df = df.select_dtypes(include=['object'])\n",
    "    numerical_df = df.select_dtypes(exclude=['object'])\n",
    "\n",
    "    \n",
    "    # replace missing values in categorical columns with the most frequent value except for the target column\n",
    "    for column in categorical_df.columns:\n",
    "        categorical_df[column]=categorical_df[column].fillna(categorical_df[column].value_counts().index[0])\n",
    "\n",
    "    # replace missing values in numerical columns with the mean except for the target column\n",
    "    for column in numerical_df.columns:\n",
    "        numerical_df[column]=numerical_df[column].fillna(numerical_df[column].mean())\n",
    "\n",
    "    \n",
    "    return categorical_df, numerical_df,target_column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_2_preprocessing():\n",
    "    # Load data\n",
    "    import numpy as np\n",
    "    df = pd.read_csv('datasets/adult/adult.data', header=None)\n",
    "    test = pd.read_csv('datasets/adult/adult.test', header=None, skiprows=1)\n",
    "    # count rows\n",
    "    df_rows = df.shape[0]\n",
    "    test_rows = test.shape[0]\n",
    "\n",
    "    # append test data to df\n",
    "    df = pd.concat([df, test], ignore_index=True)\n",
    "\n",
    "    # delete duplicates and missing values for df and test\n",
    "    df.replace(' ?', np.nan, inplace=True)\n",
    "    df.replace(' <=50K.', ' <=50K', inplace=True)\n",
    "    df.replace(' >50K.', ' >50K', inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.dropna(subset=[14], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # rename all column name from 0 to 14 \n",
    "    df.rename(columns={14: '14_column'}, inplace=True)\n",
    "    df.rename(columns={0: '0_column'}, inplace=True)\n",
    "    df.rename(columns={1: '1_column'}, inplace=True)\n",
    "    df.rename(columns={2: '2_column'}, inplace=True)\n",
    "    df.rename(columns={3: '3_column'}, inplace=True)\n",
    "    df.rename(columns={4: '4_column'}, inplace=True)\n",
    "    df.rename(columns={5: '5_column'}, inplace=True)\n",
    "    df.rename(columns={6: '6_column'}, inplace=True)\n",
    "    df.rename(columns={7: '7_column'}, inplace=True)\n",
    "    df.rename(columns={8: '8_column'}, inplace=True)\n",
    "    df.rename(columns={9: '9_column'}, inplace=True)\n",
    "    df.rename(columns={10: '10_column'}, inplace=True)\n",
    "    df.rename(columns={11: '11_column'}, inplace=True)\n",
    "    df.rename(columns={12: '12_column'}, inplace=True)\n",
    "    df.rename(columns={13: '13_column'}, inplace=True)\n",
    "\n",
    "    target_column_name = '14_column'\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # separate categorical and numerical columns\n",
    "    categorical_df = df.select_dtypes(include=['object'])\n",
    "    numerical_df = df.select_dtypes(exclude=['object'])\n",
    "\n",
    "\n",
    "    # replace missing values in categorical columns with the most frequent value except for the target column\n",
    "    for column in categorical_df.columns:\n",
    "        categorical_df[column]=categorical_df[column].fillna(categorical_df[column].value_counts().index[0])\n",
    "\n",
    "    # replace missing values in numerical columns with the mean except for the target column\n",
    "    for column in numerical_df.columns:\n",
    "        numerical_df[column]=numerical_df[column].fillna(numerical_df[column].mean())\n",
    "\n",
    "    \n",
    "\n",
    "    return categorical_df, numerical_df, target_column_name\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_3_preprocessing(small_dataset=True):\n",
    "    # Load data\n",
    "    df = pd.read_csv('datasets/creditcard.csv')\n",
    "    if small_dataset:\n",
    "        positive_data = df[df['Class'] == 1]\n",
    "        negative_data = df[df['Class'] == 0].sample(n=20000, random_state=42)\n",
    "        df = pd.concat([positive_data,negative_data],ignore_index=True)\n",
    "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "            \n",
    "    # delete duplicates and missing values\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    target_column_name = 'Class'\n",
    "    # separate categorical and numerical columns\n",
    "    categorical_df = df.select_dtypes(include=['object'])\n",
    "    numerical_df = df.select_dtypes(exclude=['object'])\n",
    "\n",
    "    # replace missing values in categorical columns with the most frequent value except for the target column\n",
    "    for column in categorical_df.columns:\n",
    "        categorical_df[column]=categorical_df[column].fillna(categorical_df[column].value_counts().index[0])\n",
    "    \n",
    "    # replace missing values in numerical columns with the mean except for the target column\n",
    "    for column in numerical_df.columns:\n",
    "        numerical_df[column]=numerical_df[column].fillna(numerical_df[column].mean())\n",
    "    \n",
    "    return categorical_df, numerical_df, target_column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df, numerical_df,target_column_name = dataset_3_preprocessing()\n",
    "#categorical_df, numerical_df,target_column_name = dataset_1_preprocessing()\n",
    "#categorical_df, numerical_df,target_column_name = dataset_2_preprocessing()\n",
    "categorical_df.info()\n",
    "numerical_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ENCODING AND SCALING***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total categories for each categorical column\n",
    "for column in categorical_df.columns:\n",
    "    print(f'{column}: {categorical_df[column].nunique()} categories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total columns count in categorical_df\n",
    "print(len(categorical_df.columns))\n",
    "categorical_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do label encodind for categorical columns if there is only 2 categories, otherwise do one hot encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# at first, do the label encoding for binary columns\n",
    "le = LabelEncoder()\n",
    "for column in categorical_df.columns:\n",
    "    if categorical_df[column].nunique() == 2:\n",
    "        categorical_df[column] = le.fit_transform(categorical_df[column])\n",
    "\n",
    "# then do one hot encoding for the rest of the columns\n",
    "if len(categorical_df.columns) > 0:\n",
    "    categorical_df = pd.get_dummies(categorical_df).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(categorical_df.columns))\n",
    "categorical_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scaling(numerical_df,categorical_df):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    # for the numerical columns\n",
    "    numerical_df = pd.DataFrame(scaler.fit_transform(numerical_df), columns=numerical_df.columns)\n",
    "    # merge them back together\n",
    "    df = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scaling(numerical_df,categorical_df):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    # for the numerical columns\n",
    "    numerical_df = pd.DataFrame(scaler.fit_transform(numerical_df), columns=numerical_df.columns)\n",
    "    # merge them back together\n",
    "    df = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_and_encoded_df = minmax_scaling(numerical_df,categorical_df)\n",
    "scaled_and_encoded_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FEATURE_SELECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_feature_selection(scaled_and_encoded_df, target_column_name, top_n_features=20):\n",
    "    X = scaled_and_encoded_df.drop(columns=[target_column_name])\n",
    "    y = scaled_and_encoded_df[target_column_name]\n",
    "    \n",
    "    correlation_matrix = X.corrwith(y)\n",
    "\n",
    "    # for feature, correlation in correlation_matrix.iteritems():\n",
    "    #     print(f'{feature}: {correlation}')\n",
    "\n",
    "    top_features = correlation_matrix.abs().sort_values(ascending=False).head(top_n_features).index\n",
    "\n",
    "    selected_columns = list(top_features) + [target_column_name]\n",
    "    return scaled_and_encoded_df[selected_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain_feature_selection(scaled_and_encoded_df,target_column_name,top_n_features=20):\n",
    "    \n",
    "\n",
    "    X = scaled_and_encoded_df.drop(columns=[target_column_name])\n",
    "    y = scaled_and_encoded_df[target_column_name]\n",
    "\n",
    "    # information gain\n",
    "    from sklearn.feature_selection import SelectKBest\n",
    "    from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "    selector = SelectKBest(mutual_info_classif, k=top_n_features)\n",
    "    selector.fit(X, y)\n",
    "\n",
    "    # get the selected feature names\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    #print(selected_features)\n",
    "    #return new df with selected features and target column\n",
    "    return pd.concat([scaled_and_encoded_df[selected_features],scaled_and_encoded_df[target_column_name]],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df = info_gain_feature_selection(scaled_and_encoded_df,target_column_name,top_n_features=20)\n",
    "selected_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df_2 = correlation_feature_selection(scaled_and_encoded_df,target_column_name,top_n_features=20)\n",
    "selected_df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***PERFORMANCE METRICS IMPLEMENTATION***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, average_precision_score\n",
    "def evaluate_model(model_name,y_true, y_pred, y_pred_prob=None,save_scores=True):\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)  # how many are correctly classified\n",
    "    sensitivity = tp / (tp + fn) # how many positive cases are correctly classified\n",
    "    specificity = tn / (tn + fp) # how many negative cases are correctly classified\n",
    "    precision = tp / (tp + fp) # how many of the positive predictions are correct\n",
    "    f1 = 2 * (precision * sensitivity) / (precision + sensitivity)\n",
    "\n",
    "\n",
    "    print(f\"========================================================Model: {model_name}===========================================================\")\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(f\"TN: {tn}, FP: {fp}\")\n",
    "    print(f\"FN: {fn}, TP: {tp}\")\n",
    "    print()\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    auroc = None\n",
    "    aupr = None\n",
    "    if y_pred_prob is not None:\n",
    "        auroc = roc_auc_score(y_true, y_pred_prob)\n",
    "        print(f\"AUROC: {auroc:.4f}\")\n",
    "        aupr = average_precision_score(y_true, y_pred_prob)\n",
    "        print(f\"AUPR: {aupr:.4f}\")\n",
    "    \n",
    "    if save_scores==False:\n",
    "        return\n",
    "    # write all the scores in a csv file, if AUROC and AUPR are none, then keep the cells empty\n",
    "    scores = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'Precision': precision,\n",
    "        'F1': f1,\n",
    "        'AUROC': auroc if y_pred_prob is not None else None,\n",
    "        'AUPR': aupr if y_pred_prob is not None else None\n",
    "    }\n",
    "\n",
    "    # draw violin plot for the scores\n",
    "\n",
    "\n",
    "    #write in csv, if the file does not exist, then create it and write the header\n",
    "    import os\n",
    "    # if the model name is StackEnsemble then write the scores in a separate file named final_scores.csv\n",
    "    if model_name == 'StackEnsemble':\n",
    "        if not os.path.exists('final_scores.csv'):\n",
    "            scores_df = pd.DataFrame([scores])\n",
    "            scores_df.to_csv('final_scores.csv', index=False)\n",
    "        else:\n",
    "            scores_df = pd.read_csv('final_scores.csv')\n",
    "            scores_df = pd.concat([scores_df, pd.DataFrame([scores])], ignore_index=True)\n",
    "            \n",
    "            scores_df.to_csv('final_scores.csv', index=False)\n",
    "        return\n",
    "    \n",
    "    # do same for VotingEnsemble\n",
    "    if model_name == 'VotingEnsemble':\n",
    "        if not os.path.exists('final_scores.csv'):\n",
    "            scores_df = pd.DataFrame([scores])\n",
    "            scores_df.to_csv('final_scores.csv', index=False)\n",
    "        else:\n",
    "            scores_df = pd.read_csv('final_scores.csv')\n",
    "            scores_df = pd.concat([scores_df, pd.DataFrame([scores])], ignore_index=True)\n",
    "            \n",
    "            scores_df.to_csv('final_scores.csv', index=False)\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists('LR_scores.csv'):\n",
    "        scores_df = pd.DataFrame([scores])\n",
    "        scores_df.to_csv('LR_scores.csv', index=False)\n",
    "    else:\n",
    "        scores_df = pd.read_csv('LR_scores.csv')\n",
    "        scores_df = pd.concat([scores_df, pd.DataFrame([scores])], ignore_index=True)\n",
    "        \n",
    "        scores_df.to_csv('LR_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# about roc and pr curve-->\n",
    "# the model gives the probability of the positive class for each sample.\n",
    "# we define a threshold to convert these probabilities to class labels. ( normally 0.5)\n",
    "# we can change the threshold to get different confusion matrix values\n",
    "# but we can't change the threshold to get different roc and pr curve values\n",
    "# because roc and pr curve are plotted by changing the threshold from 0 to 1\n",
    "# roc curve is plotted with TPR = TP / (TP + FN) vs FPR = FP / (FP + TN) for every threshold value\n",
    "# pr curve is plotted with precision = TP / (TP + FP) and recall = TP / (TP + FN) for every threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def violin_plot():\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "\n",
    "    df = pd.read_csv('LR_scores.csv')  # Replace 'your_file.csv' with the path to your CSV\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Model as id_vars, will remain fixed\n",
    "    # columns as var_name, will be melted into a single column\n",
    "    # values as value_name, will be the values in the melted\n",
    "    df_melted = pd.melt(df, id_vars=[\"Model\"], \n",
    "                        var_name=\"Metric\", value_name=\"Score\")\n",
    "    df_melted.head()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.violinplot(x='Metric', y='Score', data=df_melted, palette='Set2')\n",
    "\n",
    "\n",
    "    plt.title('Violin Plot of Model Metrics Across Different Logistic Regression Models')\n",
    "    plt.xticks(rotation=45) \n",
    "    plt.show()\n",
    "\n",
    "    df=df.drop(columns=['Model'])\n",
    "    mean_scores = df.mean()\n",
    "    standard_deviation = df.std()\n",
    "\n",
    "    # mean scores has a list of mean values for each metric. add a new column for column names at first, the mean values are the other columns\n",
    "    scores = {\n",
    "        'Model': 'LR',\n",
    "        'Accuracy': f\"{mean_scores['Accuracy']:.2f} ± {standard_deviation['Accuracy']:.2f}\",\n",
    "        'Sensitivity': f\"{mean_scores['Sensitivity']:.2f} ± {standard_deviation['Sensitivity']:.2f}\",\n",
    "        'Specificity': f\"{mean_scores['Specificity']:.2f} ± {standard_deviation['Specificity']:.2f}\",\n",
    "        'Precision': f\"{mean_scores['Precision']:.2f} ± {standard_deviation['Precision']:.2f}\",\n",
    "        'F1': f\"{mean_scores['F1']:.2f} ± {standard_deviation['F1']:.2f}\",\n",
    "        'AUROC': f\"{mean_scores['AUROC']:.2f} ± {standard_deviation['AUROC']:.2f}\",\n",
    "        'AUPR': f\"{mean_scores['AUPR']:.2f} ± {standard_deviation['AUPR']:.2f}\"\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    # write the mean values as a row in a new csv file\n",
    "    if not os.path.exists('mean_scores.csv'):\n",
    "        scores_df = pd.DataFrame([scores])\n",
    "        scores_df.to_csv('mean_scores.csv', index=False)\n",
    "    else:\n",
    "        scores_df = pd.read_csv('mean_scores.csv')\n",
    "        scores_df = pd.concat([scores_df, pd.DataFrame([scores])], ignore_index=True)\n",
    "        \n",
    "        scores_df.to_csv('mean_scores.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL IMPLEMENTATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "class MyLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, iterations=1000, regularization=None, strength=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.regularization = regularization\n",
    "        self.regularization_strength = strength\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.theta = np.zeros(X.shape[1]) # theta is a vector of zeros with the same size as the number of features\n",
    "        self.bias = 0 # bias is initialized to 0\n",
    "        m = X.shape[0] # number of samples\n",
    "\n",
    "        for _ in range(self.iterations):\n",
    "            linear_model = np.dot(X, self.theta) + self.bias # z = X.theta + b\n",
    "            predictions = sigmoid(linear_model) # a = sigmoid(z)\n",
    "            \n",
    "            dw = (1 / m) * np.dot(X.T, (predictions - y)) # X.T is the transpose of X. And np.dot returns an array with the size of the number of features\n",
    "            db = (1 / m) * np.sum(predictions - y)\n",
    "\n",
    "            if self.regularization == 'l1':\n",
    "                dw += (self.regularization_strength / m) * np.sign(self.theta) # derivative of L1 regularization = the sign of theta * the regularization strength * 1/m\n",
    "            elif self.regularization == 'l2':\n",
    "                dw += (self.regularization_strength / m) * self.theta # derivative of L2 regularization = theta  * the regularization strength * 1/m\n",
    "            \n",
    "            self.theta -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.theta) + self.bias\n",
    "        predictions = sigmoid(linear_model)\n",
    "        return [1 if i > 0.5 else 0 for i in predictions]\n",
    "    # return prediction probabilities\n",
    "    def predict_proba(self, X):\n",
    "        linear_model = np.dot(X, self.theta) + self.bias\n",
    "        predictions = sigmoid(linear_model)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "def bagging_models(X_train, y_train,n_models=9,regularization=None,strength=0.01):\n",
    "    models=[]\n",
    "    for i in range(n_models):\n",
    "        X_resampled, y_resampled = resample(X_train, y_train, random_state=i)\n",
    "        \n",
    "        model = MyLogisticRegression(regularization=regularization, strength=strength)\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        models.append(model)\n",
    "\n",
    "    return models\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_stack_ensembling(scaled_and_encoded_df, target_column_name, n_models=9, regularization=None, strength=0.01):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X = scaled_and_encoded_df.drop(columns=[target_column_name])\n",
    "    y = scaled_and_encoded_df[target_column_name]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=73)\n",
    "    # validation set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=73)\n",
    "\n",
    "\n",
    "    # for meta learner, both the predictions of the base models and base model features are the features\n",
    "\n",
    "    \n",
    "    #--------------------------------------------fitting----------------------------------------------\n",
    "    models=bagging_models(X_train, y_train,n_models=n_models,regularization=regularization,strength=strength)\n",
    "\n",
    "    # get the predictions of the base models on validation set\n",
    "    validation_preds=[]\n",
    "    for model in models:\n",
    "        validation_preds.append(model.predict(X_val))\n",
    "\n",
    "    # validation_preds has shape (n_models, n_samples)... we need to transpose it to (n_samples, n_models)\n",
    "    validation_preds = np.array(validation_preds).T\n",
    "    # merge the predictions with the validation train for meta model\n",
    "    X_meta_train = np.concatenate([X_val, validation_preds], axis=1)\n",
    "\n",
    "    # fit the meta learner\n",
    "    meta_learner = MyLogisticRegression()\n",
    "    meta_learner.fit(X_meta_train, y_val)\n",
    "\n",
    "    #--------------------------------------------predicting----------------------------------------------\n",
    "    # get the predictions of the base models on test set\n",
    "    test_preds=[]\n",
    "    for model in models:\n",
    "        test_preds.append(model.predict(X_test))\n",
    "    \n",
    "    # test_preds has shape (n_models, n_samples)... we need to transpose it to (n_samples, n_models)\n",
    "    test_preds = np.array(test_preds).T\n",
    "    # merge the predictions with the test set for meta model\n",
    "    X_meta_test = np.concatenate([X_test, test_preds], axis=1)\n",
    "\n",
    "    # predict with the meta learner\n",
    "    y_pred = meta_learner.predict(X_meta_test)\n",
    "    y_pred_prob = meta_learner.predict_proba(X_meta_test)\n",
    "    \n",
    "    return y_test, y_pred, y_pred_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_voting_ensembling(scaled_and_encoded_df, target_column_name, n_models=9, regularization=None, strength=0.01,soft_voting=True):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    from sklearn.utils import resample\n",
    "    X = scaled_and_encoded_df.drop(columns=[target_column_name])\n",
    "    y = scaled_and_encoded_df[target_column_name]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=73)\n",
    "\n",
    "    #--------------------------------------------fitting----------------------------------------------\n",
    "    models=bagging_models(X_train, y_train,n_models=n_models,regularization=regularization,strength=strength)\n",
    "\n",
    "    #--------------------------------------------predicting----------------------------------------------\n",
    "\n",
    "    # find y_pred and y_pred_prob for each model\n",
    "    y_pred_probs = []\n",
    "    for model in models:\n",
    "        y_pred_probs.append(model.predict_proba(X_test))\n",
    "    \n",
    "    y_preds= []\n",
    "    for y_pred_prob in y_pred_probs:\n",
    "        y_preds.append([1 if i > 0.5 else 0 for i in y_pred_prob])\n",
    "    \n",
    "    # evaluate each model\n",
    "    for i in range(n_models):\n",
    "        evaluate_model(f'LR_Model_{i}',y_test, y_preds[i], y_pred_probs[i],save_scores=True)\n",
    "\n",
    "\n",
    "    #--------------------------------------------voting----------------------------------------------\n",
    "\n",
    "    # find the final prediction by voting\n",
    "\n",
    "    if soft_voting:\n",
    "        # soft voting\n",
    "\n",
    "        y_pred_probs = np.array(y_pred_probs)\n",
    "        y_pred_prob = np.mean(y_pred_probs, axis=0)\n",
    "        y_pred = [1 if i > 0.5 else 0 for i in y_pred_prob]\n",
    "\n",
    "    else:\n",
    "        # hard voting \n",
    "        y_pred = []\n",
    "        for i in range(len(y_test)):\n",
    "            y_pred.append(np.argmax(np.bincount([y_preds[j][i] for j in range(n_models)])))\n",
    "        y_pred = np.array(y_pred)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return y_test, y_pred, y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # #predict with stack ensembling----------------------------------------------------------\n",
    "y_test,y_pred, y_pred_prob = predict_with_stack_ensembling(selected_df, target_column_name, n_models=9)\n",
    "\n",
    "# get the accuracy score\n",
    "\n",
    "evaluate_model('StackEnsemble',y_test, y_pred, y_pred_prob,save_scores=True)\n",
    "\n",
    "\n",
    "\n",
    "# predict with voting ensembling----------------------------------------------------------\n",
    "y_test,y_pred, y_pred_prob = predict_with_voting_ensembling(selected_df, target_column_name, n_models=9,soft_voting=True)\n",
    "\n",
    "# get the accuracy score\n",
    "\n",
    "evaluate_model('VotingEnsemble',y_test, y_pred, y_pred_prob,save_scores=True)\n",
    "violin_plot()\n",
    "\n",
    "# delete the scores file\n",
    "import os\n",
    "os.remove('LR_scores.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
