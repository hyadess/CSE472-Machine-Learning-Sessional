{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None  # First moment estimate\n",
    "        self.v = None  # Second moment estimate\n",
    "        self.t = 0     # Timestep\n",
    "\n",
    "    def update(self, weights, gradients):\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(weights)\n",
    "            self.v = np.zeros_like(weights)\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * gradients\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (gradients ** 2)\n",
    "\n",
    "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
    "\n",
    "        weights -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "        return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size,learning_rate):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_optimizer = AdamOptimizer(learning_rate)\n",
    "        self.bias_optimizer = AdamOptimizer(learning_rate)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        # Y = XW + b \n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        \n",
    "        grad_weights = np.dot(self.input.T, grad_output)\n",
    "        grad_bias = np.sum(grad_output, axis=0, keepdims=True)\n",
    "\n",
    "        #update using adam optimizer\n",
    "        self.weights = self.weight_optimizer.update(self.weights, grad_weights)\n",
    "        self.bias = self.bias_optimizer.update(self.bias, grad_bias)\n",
    "\n",
    "        # self.weights -= self.learning_rate * grad_weights\n",
    "        # self.bias -= self.learning_rate * grad_bias\n",
    "\n",
    "\n",
    "        \n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BatchNormalization:\n",
    "    def __init__(self, units,learning_rate, momentum=0.9, epsilon=1e-5):\n",
    "        self.gamma = np.ones((1, units))\n",
    "        self.beta = np.zeros((1, units))\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.running_mean = None\n",
    "        self.running_var = None\n",
    "        self.learning_rate = learning_rate\n",
    "        # initialize adam optimizer for gamma and beta\n",
    "        self.gamma_optimizer = AdamOptimizer(learning_rate)\n",
    "        self.beta_optimizer = AdamOptimizer(learning_rate)\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        if self.running_mean is None:\n",
    "            self.running_mean = np.mean(X, axis=0)\n",
    "            self.running_var = np.var(X, axis=0)\n",
    "        \n",
    "        if training:\n",
    "            # axis=0 means we are calculating mean and variance for each feature/units\n",
    "            batch_mean = np.mean(X, axis=0)\n",
    "            batch_var = np.var(X, axis=0)\n",
    "\n",
    "            # X and batch_mean have shape (batch_size, units) and (1, units)\n",
    "            # we need to broadcast batch_mean to shape (batch_size, units)\n",
    "            # batch_var+epsilon has shape (1, units), we need to broadcast it to (batch_size, units)\n",
    "            # self.normalized has shape (batch_size, units)\n",
    "            self.normalized = (X - batch_mean) / np.sqrt(batch_var + self.epsilon)\n",
    "\n",
    "            # self_gemma and self_beta have shape (1, units), we need to broadcast them to (batch_size, units)\n",
    "            # gemma * normalized is element-wise multiplication, \n",
    "            # for example: [[1, 2], [3, 4]] * [[5, 6], [7, 8]] = [[5, 12], [21, 32]] \n",
    "            # self.out has shape (batch_size, units)\n",
    "            self.out = self.gamma * self.normalized + self.beta\n",
    "\n",
    "            # Update running mean and variance\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n",
    "        else:\n",
    "            # Use running mean and variance during inference\n",
    "            self.normalized = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            self.out = self.gamma * self.normalized + self.beta\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Gradients for gamma and beta (trainable parameters)\n",
    "        grad_gamma = np.sum(grad_output * self.normalized, axis=0, keepdims=True)\n",
    "        grad_beta = np.sum(grad_output, axis=0, keepdims=True)\n",
    "\n",
    "        # Compute grad_input before updating gamma and beta\n",
    "        batch_size = grad_output.shape[0]\n",
    "        grad_normalized = grad_output * self.gamma\n",
    "        batch_var = np.var(self.normalized, axis=0, keepdims=True)\n",
    "        grad_var = np.sum(grad_normalized * (self.normalized * -0.5) * (batch_var + self.epsilon) ** (-1.5), axis=0)\n",
    "        grad_mean = np.sum(grad_normalized * -1 / np.sqrt(batch_var + self.epsilon), axis=0) + grad_var * np.mean(-2 * self.normalized, axis=0)\n",
    "\n",
    "        grad_input = grad_normalized / np.sqrt(batch_var + self.epsilon) \\\n",
    "                     + grad_var * 2 * (self.normalized - np.mean(self.normalized, axis=0)) / batch_size \\\n",
    "                     + grad_mean / batch_size\n",
    "\n",
    "        # Update gamma and beta using Adam optimizers\n",
    "        self.gamma = self.gamma_optimizer.update(self.gamma, grad_gamma)\n",
    "        self.beta = self.beta_optimizer.update(self.beta, grad_beta)\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_input = grad_output.copy()\n",
    "        grad_input[self.input <= 0] = 0  # Only propagate where input > 0\n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            # scale by 1/(1-rate) to ensure the expected value of X remains the same\n",
    "            # self.mask has the same shape as X and each element is 0 with probability rate or 1 with probability 1-rate \n",
    "            self.mask = (np.random.rand(*X.shape) > self.rate) / (1 - self.rate)\n",
    "            # apply the mask to X , element-wise multiplication\n",
    "            return X * self.mask\n",
    "        return X\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * self.mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, logits):\n",
    "        # Apply the softmax function\n",
    "        exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Stabilize with max subtraction\n",
    "        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_input = np.empty_like(grad_output)\n",
    "        \n",
    "        for i, (softmax_output, grad) in enumerate(zip(self.output, grad_output)):\n",
    "            \n",
    "            softmax_output = softmax_output.reshape(-1, 1)\n",
    "            jacobian_matrix = np.diagflat(softmax_output) - np.dot(softmax_output, softmax_output.T)\n",
    "            grad_input[i] = np.dot(jacobian_matrix, grad)\n",
    "        \n",
    "        return grad_input\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim,learning_rate):\n",
    "        self.layers = []\n",
    "        \n",
    "        # Input to hidden layers\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.layers.append(DenseLayer(prev_dim, hidden_dim,learning_rate))\n",
    "            self.layers.append(BatchNormalization(hidden_dim,learning_rate))\n",
    "            self.layers.append(ReLU())\n",
    "            self.layers.append(Dropout(rate=0.4))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Hidden to output layer\n",
    "        self.layers.append(DenseLayer(prev_dim, output_dim,learning_rate))\n",
    "        self.layers.append(Softmax())\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, (Dropout, BatchNormalization)):\n",
    "                X = layer.forward(X, training)\n",
    "            else:\n",
    "                X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, data_loader):\n",
    "    # Iterate over all batches and store losses in a list\n",
    "\n",
    "    losses = []\n",
    "    for batch, (images, labels) in enumerate(data_loader):\n",
    "        images = images.view(images.size(0), -1).numpy()  # Flatten 28x28 images\n",
    "        labels = labels.numpy()\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model.forward(images, training=True)\n",
    "\n",
    "        # Assuming `num_classes` is 10 for FashionMNIST\n",
    "        one_hot_labels = one_hot_encode(labels, num_classes=10)\n",
    "\n",
    "        \n",
    "        # Calculate loss (e.g., cross-entropy)\n",
    "        # Compute the cross-entropy loss\n",
    "        loss = -np.sum(one_hot_labels * np.log(predictions + 1e-9)) / len(labels)\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "\n",
    "        # Compute gradients and backpropagate\n",
    "        grad_output = predictions - one_hot_labels  # Gradient for softmax + cross-entropy\n",
    "        model.backward(grad_output)\n",
    "        \n",
    "        # print(f\"Batch {batch}, Loss: {loss}\")\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    # TP, TN, FP, FN for each class separately\n",
    "\n",
    "    # 2d confusion matrix for multi-class classification\n",
    "\n",
    "    confusion_matrix = np.zeros((10, 10))\n",
    "\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for images, labels in data_loader:\n",
    "        images = images.view(images.size(0), -1).numpy()\n",
    "        labels = labels.numpy() \n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model.forward(images, training=False)\n",
    "\n",
    "        predictions = np.argmax(outputs, axis=1) # Predicted class index \n",
    "\n",
    "        # Update confusion matrix\n",
    "        for i in range(len(labels)):\n",
    "            confusion_matrix[labels[i], predictions[i]] += 1\n",
    "\n",
    "        total += len(labels)    \n",
    "        correct += np.sum(predictions == labels)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / total\n",
    "    tp = np.diag(confusion_matrix)\n",
    "\n",
    "    fp = np.sum(confusion_matrix, axis=0) - tp\n",
    "\n",
    "    fn = np.sum(confusion_matrix, axis=1) - tp\n",
    "\n",
    "    total_tp = np.sum(tp)\n",
    "    total_fp = np.sum(fp)\n",
    "    total_fn = np.sum(fn)\n",
    "    # Macro F1 score\n",
    "    precisions = total_tp / (total_tp + total_fp)\n",
    "    recalls = total_tp / (total_tp + total_fn)\n",
    "    f1_scores = 2 * precisions * recalls / (precisions + recalls)\n",
    "    macro_f1 = np.mean(f1_scores)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return macro_f1, accuracy, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD THE DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "# Define transforms for data normalization and augmentation if desired\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5,), (0.5,))  # Normalize grayscale channel\n",
    "])\n",
    "\n",
    "# Download and load the FashionMNIST dataset\n",
    "train_data = FashionMNIST(root='data', train=True, transform=transform, download=True)\n",
    "# test_data = FashionMNIST(root='data', train=False, transform=transform, download=True)\n",
    "with open('b1.pkl', 'rb') as b1:\n",
    "  test_data = pickle.load(b1)\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "train_size = int(0.8 * len(train_data))\n",
    "val_size = len(train_data) - train_size\n",
    "train_dataset, val_dataset = random_split(train_data, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model_weights(model, filename):\n",
    "    weights = []\n",
    "    for layer in model.layers:\n",
    "        layer_data = {}\n",
    "        for attr in ['weights', 'bias', 'gamma', 'beta', 'running_mean', 'running_var']:\n",
    "            if hasattr(layer, attr):\n",
    "                layer_data[attr] = getattr(layer, attr)\n",
    "        weights.append(layer_data)\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(weights, f)\n",
    "\n",
    "\n",
    "def load_model_weights(model, filename):\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "    \n",
    "    for layer, layer_data in zip(model.layers, weights):\n",
    "        for attr, value in layer_data.items():\n",
    "            if hasattr(layer, attr):\n",
    "                setattr(layer, attr, value)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================Learning Rate: 0.001=======================================\n",
      "Testing on test data=======================:\n",
      "Macro F1: 0.23899196898366104\n",
      "Accuracy: 0.23899196898366104\n",
      "Confusion Matrix: \n",
      "    0     0     4     0     2   602    46     0     0     0\n",
      "    0    10     2     1     0   482     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0   853     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0  1609     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dim = 28 * 28  \n",
    "hidden_dims = [128,64]  \n",
    "output_dim = 10  \n",
    "\n",
    "\n",
    "# learning_rates = [0.003,0.002,0.001,0.0005]\n",
    "learning_rates = [0.001]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    print(f\"========================================Learning Rate: {learning_rate}=======================================\")\n",
    "    # Initialize model\n",
    "    model = NeuralNetwork(input_dim, hidden_dims, output_dim,learning_rate)\n",
    "\n",
    "    load_model_weights(model, \"model_1905064.pkl\")\n",
    "\n",
    "\n",
    "    # epochs = 25\n",
    "    # best_macro_f1 = 0\n",
    "    # best_accuracy = 0\n",
    "    # best_confusion_matrix = None\n",
    "\n",
    "    # losses = []\n",
    "    # accuracies = []\n",
    "    # f1_scores = []\n",
    "    \n",
    "    # for epoch in range(epochs):\n",
    "    #     train_loss = train(model, train_loader)\n",
    "    #     losses.append(train_loss)\n",
    "\n",
    "    #     # Evaluate on validation set\n",
    "    #     macro_f1, accuracy, confusion_matrix = evaluate(model, val_loader)\n",
    "\n",
    "    #     accuracies.append(accuracy)\n",
    "    #     f1_scores.append(macro_f1)\n",
    "\n",
    "    #     if macro_f1 > best_macro_f1:\n",
    "    #         best_macro_f1 = macro_f1\n",
    "    #         best_accuracy = accuracy\n",
    "    #         best_confusion_matrix = confusion_matrix\n",
    "\n",
    "    # # Save model weights\n",
    "    # if learning_rate == 0.001 and hidden_dims == [128,64]:\n",
    "    #     save_model_weights(model, \"model_1905064.pkl\")\n",
    "            \n",
    "\n",
    "\n",
    "    # print(\"Best Validation=========: \")\n",
    "\n",
    "    # print(f\"Macro F1: {best_macro_f1}\")\n",
    "    # # confusion matrix\n",
    "    # print(f\"Accuracy: {best_accuracy}\")\n",
    "\n",
    "    # print(\"Confusion Matrix: \")\n",
    "\n",
    "    # # print confusion matrix in a nice format (not with e, with clear numbers)\n",
    "    # for row in best_confusion_matrix:\n",
    "    #     print(' '.join([f\"{int(cell):5d}\" for cell in row]))   \n",
    "\n",
    "    # print(\"graphs=======================:\")\n",
    "\n",
    "    # import matplotlib.pyplot as plt\n",
    "\n",
    "    # # 3 graphs: Loss vs epochs, Accuracy vs epochs, F1 score vs epochs\n",
    "\n",
    "    # plt.figure(figsize=(12, 4))\n",
    "    # plt.subplot(1, 3, 1)\n",
    "    # plt.plot(losses)\n",
    "    # plt.title('Loss vs epochs')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Loss')\n",
    "\n",
    "    # plt.subplot(1, 3, 2)\n",
    "    # plt.plot(accuracies)\n",
    "    # plt.title('Accuracy vs epochs')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Accuracy')\n",
    "\n",
    "    # plt.subplot(1, 3, 3)\n",
    "    # plt.plot(f1_scores)\n",
    "    # plt.title('F1 score vs epochs')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('F1 score')\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Testing on test data=======================:\")\n",
    "    macro_f1, accuracy, confusion_matrix = evaluate(model, test_loader)\n",
    "    print(f\"Macro F1: {macro_f1}\")\n",
    "    #accuracy\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    # confusion matrix\n",
    "    print(\"Confusion Matrix: \")\n",
    "    for row in confusion_matrix:\n",
    "        print(' '.join([f\"{int(cell):5d}\" for cell in row])) \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
