{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = './resources/dataset.csv'\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns = df.isnull().sum()\n",
    "null_columns = null_columns[null_columns > 0]\n",
    "print(null_columns)\n",
    "# null_columns means the columns having null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the missing values in the dataset\n",
    "nan_columns = df.isna().count(True)\n",
    "#nan_columns = nan_columns[nan_columns > 0]\n",
    "print(nan_columns)\n",
    "# nan_columns means the columns having missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_count = df.duplicated().sum()\n",
    "print(duplicate_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B....CLEANING  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the rows if the row has missing value in attrition column\n",
    "df = df.dropna(subset=['Attrition'])\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing values with the mean of the column for other colummns\n",
    "df = df.fillna(df.mean(numeric_only=True))\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C......SPLITING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = df.drop(columns=['Attrition'])  \n",
    "label_df = df['Attrition'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the feature_df into categorical and numerical features\n",
    "categorical_feature_df = feature_df.select_dtypes(include=['object']).copy()\n",
    "categorical_feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_feature_df = feature_df.select_dtypes(exclude=['object']).copy()\n",
    "numerical_feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D......ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_one_hot_encoded = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of catergorical columns where the columns are converted into neumerical values\n",
    "def one_hot_encoding(df):\n",
    "    #catergorical columns\n",
    "    cat_columns = df.select_dtypes(include=['object']).columns\n",
    "    #print(cat_columns)\n",
    "    # print the number of values for those columns\n",
    "    print(df[cat_columns].nunique())\n",
    "\n",
    "    # encode such columns that have more than 2 values\n",
    "    one_hot_columns = df[cat_columns].nunique()[df[cat_columns].nunique() > 2].index\n",
    "    #other columns will be label encoded\n",
    "    label_columns = df[cat_columns].nunique()[df[cat_columns].nunique() <= 2].index\n",
    "\n",
    "    # print the columns that will be one hot encoded\n",
    "    print(\"columns that will be one hot encoded:\",one_hot_columns)\n",
    "\n",
    "    # at first encode the label columns\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    for column in label_columns:\n",
    "        df[column] = label_encoder.fit_transform(df[column])\n",
    "    \n",
    "    # now encode the one hot columns\n",
    "    encoded_df = pd.get_dummies(df, columns=one_hot_columns)\n",
    "\n",
    "    # print total number of catergorical columns that will be one hot encoded\n",
    "    print(\"total encoded columns created:\",df[cat_columns].nunique().sum())\n",
    "    \n",
    "    return encoded_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding(df):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    encoder = LabelEncoder()\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category', 'bool']).columns\n",
    "    #print(categorical_cols)\n",
    "    # a new df to store the encoded values\n",
    "    encoded_df = df.copy()\n",
    "    # which columns are being encoded and which value is encoded into which, also should be printed\n",
    "    for col in categorical_cols:\n",
    "        encoded_df[col] = encoder.fit_transform(df[col])\n",
    "        print(f'{col}: {dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))}')\n",
    "    return encoded_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_one_hot_encoded:\n",
    "    encoded_df = one_hot_encoding(feature_df)\n",
    "else:\n",
    "    encoded_df = label_encoding(feature_df)\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.....SCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary columns should not be standard scaled ( one hot enconded columns are binary columns)\n",
    "\n",
    "def standard_scaling(df):\n",
    "    # after encoding, the categorical columns are converted into numerical values, but if one hot encoding is used, the values are not scaled\n",
    "    scaler = StandardScaler()\n",
    "    # find the columns that have only 0 and 1 as values\n",
    "    binary_cols = [col for col in df.columns if df[col].nunique() == 2]\n",
    "\n",
    "    scaled_df = df.copy()\n",
    "\n",
    "    # scale EXCEPT the binary columns-------------------------------------------------------\n",
    "    \n",
    "    scaled_df.loc[:, df.columns.difference(binary_cols)] = scaler.fit_transform(df.loc[:, df.columns.difference(binary_cols)])\n",
    "\n",
    "    # scale with all columns----------------------------------------------------------------\n",
    "    #scaled_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "    # only show the columns that are scaled\n",
    "    print(scaled_df.columns.difference(binary_cols))\n",
    "    # count the number of binary columns\n",
    "    print(len(binary_cols))\n",
    "    \n",
    "\n",
    "    # return the scaled dataframe with the binary columns\n",
    "\n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    # which columns are scaled and what are the min and max values of the columns should be printed by name of every column\n",
    "    for col in df.columns:\n",
    "        print(f'{col}: min={df[col].min()}, max={df[col].max()}')\n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = standard_scaling(encoded_df)\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROCESSING THE LABEL DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make label df a pd dataframe\n",
    "label_df = pd.DataFrame(label_df)\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding od label\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "label_df['Attrition'] = encoder.fit_transform(label_df['Attrition']) \n",
    "label_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F........CORRELATION ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation values with respect to the label\n",
    "correlation = scaled_df.corrwith(label_df['Attrition'])\n",
    "print(correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the correlation values\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x=correlation.values, y=correlation.index)\n",
    "plt.title('Correlation with Attrition')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find top 20 columns with highest correlation values keep negatives ones negative\n",
    "correlation = correlation.abs().sort_values(ascending=False)\n",
    "top = correlation.head(20)\n",
    "top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the highest 20 correlation values\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x=top.values, y=top.index)\n",
    "plt.title('Top Correlations with Attrition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation matrix between the features\n",
    "\n",
    "\n",
    "# correlation_matrix = scaled_df.corr()\n",
    "# plt.figure(figsize=(20, 20))\n",
    "# sns.heatmap(correlation_matrix, annot=True, fmt='.2f')\n",
    "# plt.title('Correlation Matrix')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new df merging the scaled_df and label_df\n",
    "scattered_df = pd.concat([scaled_df, label_df], axis=1)\n",
    "scattered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "# # Select the top 20 columns with the highest correlation with the target variable\n",
    "\n",
    "\n",
    "# Separate the scattered_df based on numeric labels of label_df\n",
    "class_0 = scattered_df[scattered_df['Attrition'] == 0]\n",
    "class_1 = scattered_df[scattered_df['Attrition'] == 1]\n",
    "\n",
    "\n",
    "# 1D scatter plots for the top features for different classes of attrition\n",
    "plt.figure(figsize=(40, 40))\n",
    "for i, col in enumerate(top.index):\n",
    "    plt.subplot(5, 6, i+1)\n",
    "    plt.scatter(class_0[col], np.zeros_like(class_0[col]), label='0', alpha=0.5)\n",
    "    plt.scatter(class_1[col], np.ones_like(class_1[col]), label='1', alpha=0.5)\n",
    "    plt.xlabel(col)\n",
    "    plt.legend()\n",
    "    plt.title(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other columns except top 20 columns\n",
    "selected_features = scaled_df[top.index]\n",
    "selected_features.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G......TRAIN A MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, do logistic regression with the selected features and label\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(selected_features, label_df, test_size=0.2, random_state=4)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
